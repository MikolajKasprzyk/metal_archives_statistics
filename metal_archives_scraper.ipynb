{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MikolajKasprzyk/metal_archives_statistics/blob/main/metal_archives_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGFQuECFTXoe"
      },
      "outputs": [],
      "source": [
        "!apt update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt install firefox-geckodriver\n",
        "!cp /usr/lib/geckodriver /usr/bin\n",
        "!cp /usr/lib/firefox /usr/bin\n",
        "\n",
        "import string\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.firefox.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.firefox.service import Service\n",
        "from selenium import webdriver\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import datetime\n",
        "import pickle\n",
        "import logging\n",
        "import traceback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0g1m37P09H4"
      },
      "outputs": [],
      "source": [
        "# SET PATH TO CURRENT FOLDER\n",
        "path = '/content/drive/My Drive/Colab Notebooks/metal_archives/'\n",
        "\n",
        "# SCRAPING LIST OF BAND'S NAMES AND URLS\n",
        "\n",
        "# setting up firefox webdriver for Google Colab\n",
        "binary = '/usr/bin/firefox'\n",
        "options = webdriver.FirefoxOptions()\n",
        "options.binary = binary\n",
        "options.add_argument('start-maximized')\n",
        "options.add_argument('--headless')\n",
        "driver = webdriver.Firefox(options=options, \n",
        "                           executable_path='/usr/bin/geckodriver')\n",
        "\n",
        "# Because scraping takes time and occasional errors it is good to save to file.\n",
        "# Read file of dataframe with band's names and urls if exists:\n",
        "try:\n",
        "    bands_urls = pd.read_pickle(path + 'bands_urls.pkl')\n",
        "\n",
        "# If dataframe with bands names and urls does not exist, start scraping.\n",
        "# On Metal Archives there is alphabetical list of bands. Url strats with \n",
        "# what you see below, and ends with A-Z (or other character) letter.\n",
        "except FileNotFoundError as e:\n",
        "    logging.error(f\"File not found: {e.filename}\")\n",
        "    \n",
        "    # beginning of the url with albhabetical list - website specific\n",
        "    list_url = \"https://www.metal-archives.com/lists/\"\n",
        "    # the url ends with letter A-Z depending on first letter of the band's name\n",
        "    # which is specific for this website\n",
        "    alphabet = list(string.ascii_uppercase)\n",
        "    # besides letters there are urls for bands starting with other characters\n",
        "    alphabet.extend([\"NBR\", \"~\"])\n",
        "    # create dataframe for urls for every band in the alphabetical list\n",
        "    # with this list we can scrap data piece by piece, test it easier etc\n",
        "    bands_urls = pd.DataFrame(columns=['band_name', 'band_url'])\n",
        "\n",
        "    # SCRAPING DATAFRAME WITH NAMES AND URLS OF BANDS\n",
        "    # loop through all urls alphabetical, create list of selinium objects with\n",
        "    # band's names and urls \n",
        "    for letter in alphabet:\n",
        "        web = list_url + letter\n",
        "        driver.get(web)  # open website in the browser\n",
        "        time.sleep(3)  # load the website's content takes some time\n",
        "\n",
        "        while True:\n",
        "            # searching the html for the tag in braces (band name and url)\n",
        "            urls_selenium = driver.find_elements(By.XPATH, \"//tbody/tr/td/a\")\n",
        "            # insert found data into dataframe\n",
        "            for band in urls_selenium:\n",
        "                bands_urls.loc[len(bands_urls)] = [band.text, \n",
        "                                                   band.get_attribute(\"href\")]\n",
        "            try:\n",
        "                # looking if there is a button to click on the next site\n",
        "                # as every alphabet letter has several pages with bands listed\n",
        "                next_site = driver.find_element(\n",
        "                            By.XPATH, \"//a[@class='next paginate_button']\")\n",
        "            except Exception:\n",
        "                # if there is no button (no more table for the letter), break\n",
        "                break\n",
        "            # if there is a next site button, click it\n",
        "            next_site.click()\n",
        "            time.sleep(3)\n",
        "    \n",
        "    # write dataframe to csv to load it later - it took long time to scrape\n",
        "    bands_urls.to_pickle(path + 'bands_urls.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIQpHYjjjQWz"
      },
      "outputs": [],
      "source": [
        "# SCRAPING INFO ABOUT BANDS USING DATAFRAME WITH URLS CREATED ABOVE\n",
        "\n",
        "bands_urls.drop_duplicates(inplace=True)\n",
        "\n",
        "# whole thing is in while loop that breaks when urls ends \n",
        "# because it is sometimes necassary to restart webdriver\n",
        "\n",
        "while True:\n",
        "    print('Restarting webdriver...')\n",
        "    # set errors counter to 0, it counts consecutive errors to determine if \n",
        "    # problem is with single url or something more\n",
        "    error_count = 0\n",
        "    # setting up firefox webdriver for Google Colab\n",
        "    binary = '/usr/bin/firefox'\n",
        "    options = webdriver.FirefoxOptions()\n",
        "    options.binary = binary\n",
        "    options.add_argument('start-maximized')\n",
        "    options.add_argument('--headless')\n",
        "    driver = webdriver.Firefox(options=options, \n",
        "                               executable_path='/usr/bin/geckodriver')\n",
        "\n",
        "    # read saved file if exists, its used to scrape info piece by piece\n",
        "    try:\n",
        "        bands_info_df = pd.read_pickle(path + 'bands_info_df.pkl')\n",
        "        # last band info can be corrupted because of breaking scraping loop\n",
        "        bands_info_df.drop(bands_info_df.tail(1).index, inplace=True)\n",
        "    except FileNotFoundError as e:\n",
        "        logging.error(f\"File not found: {e.filename}\")\n",
        "        # if there is no dataframe saved, create new empty one with column to \n",
        "        # compare with when making missing_info_df\n",
        "        bands_info_df = pd.DataFrame(columns=['band_url'])\n",
        "\n",
        "    # SCRAPING BAND INFO\n",
        "    try:\n",
        "        # iterating on dataframe bands_missing_info which is made by\n",
        "        # subtracting urls of bands already in bands_info_df\n",
        "        # dataframe (read from file) from bands_urls dataframe\n",
        "        bands_missing_info = bands_urls[~bands_urls['band_url']\n",
        "                                        .isin(bands_info_df['band_url'])]\n",
        "        # Create df with two columns as apparently you can not\n",
        "        # assign list to df cell creating column at the same time\n",
        "        bands_info_scraped = pd.DataFrame(columns=['artists', 'discog'])\n",
        "\n",
        "\n",
        "        for index, row in bands_missing_info.iterrows():\n",
        "\n",
        "            band_name = row['band_name'] # get name and insert into new df\n",
        "            band_url = row['band_url'] # get name and insert into new df\n",
        "\n",
        "            bands_info_scraped.at[index, 'band_name'] = band_name\n",
        "            bands_info_scraped.at[index, 'band_url'] = band_url\n",
        "            try:\n",
        "                # if error is not single url break loop and restart driver\n",
        "                if error_count > 1:\n",
        "                    break\n",
        "                # if error was single url reset counter\n",
        "                error_count = 0\n",
        "                \n",
        "                # INFO SCRAPING\n",
        "                driver.get(band_url)\n",
        "                # needs to load page, also M-A asks 3s delay for scraping\n",
        "                time.sleep(3)\n",
        "                # press the button to show table with discography\n",
        "                button = driver.find_element(By.ID, \"ui-id-6\")\n",
        "                button.click()\n",
        "                # get html content\n",
        "                page_source = driver.page_source\n",
        "                soup = BeautifulSoup(page_source, \"lxml\")\n",
        "\n",
        "                # find the tag with all the data\n",
        "                band_info = soup.find(id=\"band_info\")\n",
        "                # create a list of all the data\n",
        "                band_stats = band_info.find_all(\"dd\")\n",
        "\n",
        "                # assign the stripped data strings dto variables\n",
        "                country = band_stats[0].text.strip()\n",
        "                status = band_stats[2].text.strip()\n",
        "                formed_in = band_stats[3].text.strip()\n",
        "                genre = band_stats[4].text.strip()\n",
        "                lyrical_themes = band_stats[5].text.strip()\n",
        "                current_label = band_stats[6].text.strip()\n",
        "                years_active = band_stats[7].text.strip()\n",
        "\n",
        "                # assign to dataframe\n",
        "                bands_info_scraped.at[index, 'country'] = country\n",
        "                bands_info_scraped.at[index, 'status'] = status\n",
        "                bands_info_scraped.at[index, 'formed_in'] = formed_in\n",
        "                bands_info_scraped.at[index, 'genre'] = genre\n",
        "                bands_info_scraped.at[index, 'lyrical_themes'] = lyrical_themes\n",
        "                bands_info_scraped.at[index, 'current_label'] = current_label\n",
        "                bands_info_scraped.at[index, 'years_active'] = years_active\n",
        "                \n",
        "                # ARTISTS\n",
        "                artists = soup.find(id=\"band_tab_members_current\")\n",
        "                artist_tags = artists.find_all('a', class_='bold')\n",
        "                artist_list = [artist.text for artist in artist_tags]\n",
        "                # assign to df\n",
        "                bands_info_scraped.at[index, 'artists'] = artist_list\n",
        "\n",
        "                # ALBUMS\n",
        "                # find discography table\n",
        "                table = soup.find('table', class_='display discog')\n",
        "                rows = table.find_all('tr') # find all rows of table\n",
        "                \n",
        "                albums = []\n",
        "                for row in rows:\n",
        "                    cols = row.find_all('td')\n",
        "                    cols = [col.text.strip() for col in cols]\n",
        "                    albums.append(cols)\n",
        "                # assign do df\n",
        "                bands_info_scraped.at[index, 'discog'] = albums\n",
        "\n",
        "                # save to file every 10 bands or when finished\n",
        "                if len(bands_info_scraped) % 10 == 0 \\\n",
        "                        or index == bands_missing_info.index[-1]:\n",
        "                    # concat scraped info to main df\n",
        "                    bands_info_df = pd.concat(\n",
        "                                    [bands_info_df, bands_info_scraped])\n",
        "                    bands_info_df.to_pickle(path + 'bands_info_df.pkl')\n",
        "                    # clear bands_info_scraped for next iteration\n",
        "                    bands_info_scraped = pd.DataFrame(\n",
        "                                         columns=['artists', 'discog'])\n",
        "                    \n",
        "            \n",
        "            except  Exception as e:\n",
        "                logging.error(traceback.format_exc())\n",
        "                # here is error counter to check if single url is problematic or\n",
        "                # there are some bigger issues - if so breaking for loop and \n",
        "                # restarting webdriver (begining of while loop)\n",
        "                error_count += 1\n",
        "                print('Problematic url:', str(band_url))\n",
        "                print('Index:  ', index)\n",
        "                continue\n",
        "            # break when finished\n",
        "            if index == bands_missing_info.index[-1]:\n",
        "                break\n",
        "    \n",
        "    except  Exception as e:\n",
        "        logging.error(traceback.format_exc())\n",
        "        continue\n",
        "    # break when finished\n",
        "    if index == bands_missing_info.index[-1]:\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1f6GpSp0cwKWKDxcXpWwiuvMSulaB6Dko",
      "authorship_tag": "ABX9TyP/Oqeh6QNPDTv/TLmtxMCf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}